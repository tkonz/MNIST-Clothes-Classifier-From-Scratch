{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LoadDataModule import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools    \n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import ceil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(48000, 1, 28, 28)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Part 1: ANN classifier\n",
    "# Load images and labels using LoadDataModule()\n",
    "ld = LoadDataModule()\n",
    "images, labels = ld.load('train')\n",
    "labels = labels.reshape(-1,1)\n",
    "\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "#y_true = to_categorical(data)\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(labels)\n",
    "y_true = enc.transform(labels).toarray()\n",
    "y_true\n",
    "\n",
    "shape = (-1, 1, 28, 28)\n",
    "from sklearn.model_selection import train_test_split\n",
    "XTrain, XValid, yTrain, yValid = train_test_split(images, y_true, test_size=0.2, shuffle=False)\n",
    "XTrain = XTrain.reshape(shape)\n",
    "XValid = XValid.reshape(shape)\n",
    "XTrain.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From sklearn's website\n",
    "# Located here: http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-selection-plot-confusion-matrix-py\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    fmt = '.2f' if normalize else 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt),\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 10)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "#y_true = to_categorical(data)\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(labels)\n",
    "y_true = enc.transform(labels).toarray()\n",
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 10)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_imgs, test_labels = ld.load('test')\n",
    "test_labels = test_labels.reshape(-1,1)\n",
    "enc.fit(test_labels)\n",
    "test_labels = enc.transform(test_labels).toarray()\n",
    "test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['T-shirt / top', \n",
    "              'Trouser',\n",
    "              'Pullover',\n",
    "              'Dress',\n",
    "              'Coat',\n",
    "              'Sandal',\n",
    "              'Shirt',\n",
    "              'Sneaker',\n",
    "              'Bag',\n",
    "              'Ankle boot']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VECTORIZE\n",
    "def get_im2col_indices(x_shape, field_height=3, field_width=3, padding=1, stride=1, kc=1):\n",
    "    # First figure out what the size of the output should be\n",
    "    N, H, W, C = x_shape\n",
    "    #print(\"N = \", N)\n",
    "    #print(\"H = \", H)\n",
    "    #print(\"W = \", W)\n",
    "    #print(\"C = \", C)\n",
    "    if (H + 2 * padding - field_height) % stride != 0:\n",
    "        H-=1\n",
    "        W-=1\n",
    "    \n",
    "    assert (H + 2 * padding - field_height) % stride == 0\n",
    "    assert (W + 2 * padding - field_height) % stride == 0\n",
    "    out_height = (H + 2 * padding - field_height) / stride + 1\n",
    "    #print(\"out_height = \", out_height)\n",
    "    out_width = (W + 2 * padding - field_width) / stride + 1\n",
    "    #print(\"out_width = \", out_width)\n",
    "    \n",
    "    #print(\"kc = \", kc)\n",
    "    i0 = np.repeat(np.arange(field_height, dtype='int32'), field_width)\n",
    "    #print(\"i0=\", i0.shape)\n",
    "    i0 = np.tile(i0, C)\n",
    "    #i0 = np.tile(i0, kc)\n",
    "    #print(\"i0 = \", i0.shape)\n",
    "    i1 = stride * np.repeat(np.arange(out_height ,dtype='int32'), out_width)\n",
    "    #print(\"i1 = \", i1.shape)\n",
    "    j0 = np.tile(np.arange(field_width), field_height * C)\n",
    "    #j0 = np.tile(np.arange(field_width), field_height * kc)\n",
    "    #print(\"j0 = \", j0.shape)\n",
    "    j1 = stride * np.tile(np.arange(out_width,dtype='int32'), int(out_height))\n",
    "\n",
    "    i = i0.reshape(-1, 1) + i1.reshape(1, -1)\n",
    "    j = j0.reshape(-1, 1) + j1.reshape(1, -1)\n",
    "\n",
    "    k = np.repeat(np.arange(C,dtype='int32'), field_height * field_width).reshape(-1, 1)\n",
    "    #print(\"k = \", k.shape)\n",
    "\n",
    "    return (k, i, j)\n",
    "\n",
    "def im2col_indices(x, field_height=3, field_width=3, padding=1, stride=1, kc=1):\n",
    "    \"\"\" An implementation of im2col based on some fancy indexing \"\"\"\n",
    "    # Zero-pad the input\n",
    "    p = padding\n",
    "    #print(\"padding = \", p)\n",
    "    x_padded = np.pad(x, ((0, 0), (p, p), (p, p), (0, 0)), mode='constant')\n",
    "    #print(\"x_padded.shape = \", x_padded.shape)\n",
    "\n",
    "    k, i, j = get_im2col_indices(x.shape, field_height, field_width, padding, stride, kc=kc)\n",
    "    #print(\"k.shape = \", k.shape)\n",
    "    #print(\"i.shape = \", i.shape)\n",
    "    #print(\"j.shape = \", j.shape)\n",
    "\n",
    "    cols = x_padded[:, i, j, k]\n",
    "    #print(\"x.shape = \", x.shape)\n",
    "    #C = x.shape[3]\n",
    "    C = kc\n",
    "    #print(\"C = \", C)\n",
    "    cols = cols.transpose(1, 2, 0).reshape(field_height * field_width * C, -1)\n",
    "    #print(\"cols.shape = \", cols.shape, \" end of im2col back to conv\\n\")\n",
    "    return cols\n",
    "\n",
    "\n",
    "def col2im_indices(cols, x_shape, field_height=3, field_width=3, padding=1, stride=1):\n",
    "    \"\"\" An implementation of col2im based on fancy indexing and np.add.at \"\"\"\n",
    "    N, H, W, C = x_shape\n",
    "    #print(\"N = \", N)\n",
    "    #print(\"H = \", H)\n",
    "    #print(\"W = \", W)\n",
    "    #print(\"C = \", C)\n",
    "    H_padded, W_padded = H + 2 * padding, W + 2 * padding\n",
    "    x_padded = np.zeros((N, C, H_padded, W_padded), dtype=cols.dtype)\n",
    "    k, i, j = get_im2col_indices(x_shape, field_height, field_width, padding,\n",
    "                               stride)\n",
    "    cols_reshaped = cols.reshape(C * field_height * field_width, -1, N)\n",
    "    #print(\"cols_reshaped = \", cols_reshaped.shape)\n",
    "    cols_reshaped = cols_reshaped.transpose(2, 0, 1)\n",
    "    np.add.at(x_padded, (slice(None), k, i, j), cols_reshaped)\n",
    "    if padding == 0:\n",
    "        return x_padded\n",
    "    return x_padded[:, :, padding:-padding, padding:-padding]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LAYERS\n",
    "class Conv():\n",
    "    def __init__(self, X_dim, kernel_w, kernel_h, kernel_c, F_out, stride, padding):\n",
    "\n",
    "        _, self.H, self.W, self.C = X_dim\n",
    "        print(\"Conv Object is getting made\\n\")\n",
    "        #print(\"self.N = \", self.N)\n",
    "        print(\"self.H = \", self.H)\n",
    "        print(\"self.W = \", self.W)\n",
    "        print(\"self.C = \", self.C)\n",
    "\n",
    "        self.F_out = F_out\n",
    "        #print(\"self.F_out = \", F_out)\n",
    "\n",
    "        self.kh, self.kw, self.kc = kernel_h, kernel_w, kernel_c\n",
    "        self.stride, self.padding = stride, padding\n",
    "\n",
    "        self.weights = np.random.randn(self.F_out, kernel_h, kernel_w, kernel_c) / np.sqrt(self.kc / 2.)\n",
    "        self.b = np.zeros((self.F_out,1))\n",
    "        #self.b = np.zeros((self.kc,1))\n",
    "        self.params = [self.weights, self.b]\n",
    "        #print(self.weights)\n",
    "\n",
    "        self.h_out = (self.H - kernel_h + 2 * padding) // stride + 1\n",
    "        self.w_out = (self.W - kernel_w + 2 * padding) // stride + 1\n",
    "        #print(\"self.h_out = \", self.h_out)\n",
    "        #print(\"self.w_out = \", self.w_out)\n",
    "        \n",
    "        \n",
    "        #if not self.h_out.is_integer() or not self.w_out.is_integer():\n",
    "        #    raise Exception(\"Invalid dimensions!\")\n",
    "\n",
    "        #self.h_out, self.w_out = self.h_out, self.w_out\n",
    "        self.out_dim = (self.kc, self.h_out, self.w_out)\n",
    "        \n",
    "    def toString(self):\n",
    "        \n",
    "        print(\"\\nOther hyperparams:\\n X_dim(i.e., channels) = (\", str(self.N),\n",
    "              \", \", str(self.C), \", \", str(self.H), \", \", str(self.W), \")\\nNum Filters: \", \n",
    "              str(self.kc), \"\\nFilter Height: \", str(self.kh), \"\\nFilter Width: \", \n",
    "              str(self.kw), \"\\nStride: \", str(self.stride), \"\\nPadding: \", str(self.padding), \n",
    "              \"\\nW.shape[0]: \", str(self.weights.shape[0]), \"\\nW.shape[1]: \", str(self.weights.shape[1]), \n",
    "              \"\\nW.shape[2]: \", str(self.weights.shape[2]), \"\\nb.shape[0]: \",\n",
    "              str(self.b.shape[0]), \"\\nb.shape[1]: \", str(self.b.shape[1]), \"\\nout_dim[0]: \",\n",
    "              str(self.out_dim[0]), \"\\nout_dim[1]: \", str(self.out_dim[1]), \"\\nout_dim[2]: \", str(self.out_dim[2]))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Inputs:\n",
    "        x = Input data of shape (N, H, W, C)\n",
    "        self.W = Filter weights of shape (F, kh, kw, kc)\n",
    "        self.b = Biases of shape (F,)\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "    \n",
    "        Outputs:\n",
    "        out = output data of shape (N, F, H', W')\n",
    "        H' = 1 + (H + 2*pad - kh) / stride\n",
    "        W' = 1 + (W + 2*pad - kw) / stride\n",
    "        cache = (x, w, b, conv_param)\n",
    "        '''\n",
    "    \n",
    "        #print(\"Start of Conv Forward\\n\")\n",
    "        out = None\n",
    "        pad_num = self.padding\n",
    "        stride = self.stride\n",
    "        F = self.F_out\n",
    "        #print(\"F = \", F)\n",
    "        \n",
    "        N, H, W, C = x.shape\n",
    "        self.N = N\n",
    "        #print(\"N = \", N)\n",
    "        #print(\"H = \", H)\n",
    "        #print(\"W = \", W)\n",
    "        #print(\"C = \", C)\n",
    "        \n",
    "        H_prime = 1 + (H + 2*pad_num - self.kh) / stride\n",
    "        W_prime = 1 + (W + 2*pad_num - self.kw) / stride\n",
    "        \n",
    "        x_reshaped = x.reshape(x.shape[0]*x.shape[3], x.shape[1], x.shape[2], 1)\n",
    "\n",
    "        #print(\"To im2col_indices\\n\")\n",
    "        self.x_col = im2col_indices(x_reshaped, self.kh, self.kw, stride=self.stride, padding=self.padding, kc=self.kc)\n",
    "        #print(\"Back from im2col_indices\\n\")\n",
    "        #print(\"x_col.shape = \", self.x_col.shape)\n",
    "        \n",
    "        w_row = self.weights.reshape(F, -1)\n",
    "        #print(\"w_row.shape = \", w_row.shape)\n",
    "        \n",
    "        out = w_row @ self.x_col + self.b\n",
    "        #print(\"out shape before reshaping = \", out.shape)\n",
    "        \n",
    "        out = out.reshape(N, self.h_out, self.w_out, F) \n",
    "        #print(\"out shape after reshaping to 2D = \", out.shape, \"\\nend of Conv.forward()\\n\\n\")\n",
    "\n",
    "        \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        '''\n",
    "        Inputs: \n",
    "        dout = upstream derivatives\n",
    "        cache = a tuple of (x, w, b, conv_param)\n",
    "    \n",
    "        Output:\n",
    "        dx = gradient with respect to x\n",
    "        dw = gradient with respect to w\n",
    "        db = gradient with respect to b\n",
    "        '''\n",
    "    \n",
    "        dout_flat = dout.transpose(3, 1, 2, 0).reshape(self.F_out, -1)\n",
    "        dw = dout_flat @ self.x_col.T\n",
    "        dw = dw.reshape(self.weights.shape)\n",
    "        #print(\"dw.shape = \", dw.shape)\n",
    "        #db = np.sum(dout, axis=(1,2))\n",
    "        #print(db.shape)\n",
    "        db = np.sum(dout, axis=(0, 1, 2)).reshape(self.F_out,-1)\n",
    "        #print(\"db.shape = \", db.shape)\n",
    "        \n",
    "        w_flat = self.weights.reshape(self.F_out, -1)\n",
    "        dx_col = w_flat.T @ dout_flat\n",
    "        shape = (self.N, self.H, self.W, self.C)\n",
    "        #print(\"Conv backward shape = \", shape)\n",
    "        dx = col2im_indices(dx_col, shape, self.kh, self.kw, self.padding, self.stride)\n",
    "        \n",
    "        return dx, [dw, db]\n",
    "    \n",
    "\n",
    "class Maxpool():\n",
    "    def __init__(self, X_dim, size, stride):\n",
    "        _, self.H, self.W, self.C = X_dim\n",
    "        self.params = []\n",
    "\n",
    "        self.size = size\n",
    "        self.stride = stride\n",
    "                          \n",
    "        self.h_out = (self.H - size) // stride + 1\n",
    "        #print(\"self.h_out = \", self.h_out)\n",
    "        self.w_out = (self.W - size) // stride + 1\n",
    "        #print(\"self.w_out = \", self.h_out)\n",
    "\n",
    "\n",
    "        self.h_out, self.w_out = int(self.h_out), int(self.w_out)\n",
    "        self.out_dim = (self.C, self.h_out, self.w_out)\n",
    "                          \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Inputs: \n",
    "        x = 4D tensor of shape (N, H, W, C)\n",
    "        self.size\n",
    "    \n",
    "        Output:\n",
    "        out = pooled output\n",
    "        cache = input x and pool_param\n",
    "        '''\n",
    "        self.x = x\n",
    "        N, H, W, C = x.shape\n",
    "        self.N = N\n",
    "        #print(\"N = \", N)\n",
    "        #print(\"H = \", H)\n",
    "        #print(\"W = \", W)\n",
    "        #print(\"C = \", C)\n",
    "        \n",
    "        stride = self.stride\n",
    "        x_reshaped = x.reshape(x.shape[0]*x.shape[3], x.shape[1], x.shape[2], 1)\n",
    "        self.x_col = im2col_indices(x_reshaped, self.size, self.size, padding=0, stride=self.stride)\n",
    "        #print(\"self.x_col.shape = \", self.x_col.shape)\n",
    "        \n",
    "        self.max_indices = np.argmax(self.x_col, axis=0)\n",
    "        out = self.x_col[self.max_indices, range(self.max_indices.size)]\n",
    "        \n",
    "        out = out.reshape(self.N, self.h_out, self.w_out, self.C).transpose(1,2,0,3)\n",
    "        out = out.reshape(out.shape[2], out.shape[0], out.shape[1], out.shape[3])\n",
    "        #print(out.shape)\n",
    "         \n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        '''\n",
    "        Inputs:\n",
    "        dout = upstream derivatives, same size as  cached x\n",
    "        cache = tuple of (x, pool_param)\n",
    "\n",
    "        Output:\n",
    "        dx = gradient with respect to x\n",
    "        '''\n",
    "        \n",
    "        dx_col = np.zeros_like(self.x_col)\n",
    "        dout_flat = dout.transpose(2,0,1,3).ravel()\n",
    "        \n",
    "        dx_col[self.max_indices, range(self.max_indices.size)] = dout_flat\n",
    "        \n",
    "        shape = (self.N, self.H, self.W, self.C)\n",
    "        #print(\"maxpool backward shape = \", shape)\n",
    "        dx = col2im_indices(dx_col, shape, self.size, self.size, padding=0, stride=self.stride)\n",
    "        dx = dx.reshape(self.N, self.H, self.W, self.C)\n",
    "\n",
    "        return dx, []\n",
    "\n",
    "    \n",
    "class relu():\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        \n",
    "    def forward(self, z):\n",
    "        '''\n",
    "        Input:\n",
    "        z = input of any shape\n",
    "\n",
    "        Output:\n",
    "        tuple of (out, cache)\n",
    "        '''\n",
    "        self.z = z\n",
    "        relu = lambda z: z * (z > 0)\n",
    "        out = relu(z)\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        '''\n",
    "        Input:\n",
    "        dout = post-activation gradient\n",
    "        cache = previous x used in forward prop \n",
    "\n",
    "        Output:\n",
    "        dz = gradient with respect to z\n",
    "        '''\n",
    "\n",
    "        dz, z = None, self.z\n",
    "        dz = dout * (z > 0)\n",
    "\n",
    "        assert (dz.shape == z.shape)\n",
    "\n",
    "        return dz, []\n",
    "    \n",
    "\n",
    "class sigmoid():\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        \n",
    "    def forward(self, z):\n",
    "        '''\n",
    "        Input: \n",
    "        z = input of any shape\n",
    "\n",
    "        Output:\n",
    "        tuple of (out, cache)\n",
    "        '''\n",
    "        out = 1 / (1 + np.exp(-z))\n",
    "        self.z = z\n",
    "\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        '''\n",
    "        Input:\n",
    "        dout = post-activation gradient\n",
    "        self.z = z\n",
    "\n",
    "        Output:\n",
    "        dz = gradient with respect to z\n",
    "        '''\n",
    "\n",
    "        z = self.z\n",
    "        s = 1 / (1 + np.exp(-z))\n",
    "        dz = dout * s * (1 - s)\n",
    "\n",
    "        assert (dz.shape == z.shape)\n",
    "\n",
    "        return dz, []\n",
    "\n",
    "class tanh():\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        \n",
    "    def forward(self, z):\n",
    "        '''\n",
    "        Input:\n",
    "        z = input of any shape\n",
    "\n",
    "        Output:\n",
    "        tuple of (out, cache)\n",
    "        '''\n",
    "        self.out = np.tanh(z)\n",
    "        self.z = z\n",
    "        return self.out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        '''\n",
    "        Input:\n",
    "        dout = post-activation gradient\n",
    "        self.z = z\n",
    "\n",
    "        Output:\n",
    "        dz = gradient with respect to z\n",
    "        '''\n",
    "        z = self.z\n",
    "        dz = dout * (1 - np.square(self.out))\n",
    "\n",
    "        return dz, []\n",
    "\n",
    "    \n",
    "class fully_connected():\n",
    "    def __init__(self, in_size, out_size):\n",
    "        self.W = np.random.randn(in_size, out_size) / np.sqrt(in_size / 2.)\n",
    "        self.b = np.zeros((1, out_size))\n",
    "        self.params = [self.W, self.b]\n",
    "        \n",
    "    def forward(self, x):\n",
    "        '''\n",
    "        Inputs:\n",
    "        x = input (batch size, d_1, ..., d_k)\n",
    "        self.W = weights of shape (input dim, # of outputs of fc layer)\n",
    "        self.b = bias (# of outputs of fc layer,)\n",
    "\n",
    "        Output:\n",
    "        out = output of shape (batch size, # of outputs of fc layer)\n",
    "        cache = x\n",
    "        '''\n",
    "\n",
    "        N = x.shape[0]\n",
    "\n",
    "        # 2D matrix [N, D]\n",
    "        reshape_input = x.reshape(N, -1)\n",
    "\n",
    "        out = np.dot(x,self.W) + self.b\n",
    "\n",
    "        self.x = x\n",
    "        return out\n",
    "    \n",
    "\n",
    "    def backward(self, dout):\n",
    "        '''\n",
    "        Inputs:\n",
    "        dout = partial derivative with respecty to loss of shape (N, M)\n",
    "        cache = (x,w,b)\n",
    "\n",
    "        Output:\n",
    "        dx = gradient with respect to x, of shape (N, d_1,...,d_k)\n",
    "        dw = gradient with respect to w, of shape (input dim, output of fc-layer)\n",
    "        db = gradient with respect to b, of shape (output of fc-layer,)\n",
    "        '''\n",
    "\n",
    "        x = self.x\n",
    "        w = self.W\n",
    "        b = self.b\n",
    "        \n",
    "        N = x.shape[0]\n",
    "        # dX\n",
    "        dx = np.dot(dout, w.T)\n",
    "        dx = dx.reshape(x.shape)\n",
    "\n",
    "        # dW\n",
    "        reshape_input = x.reshape(N, -1)\n",
    "        dw = reshape_input.T.dot(dout)\n",
    "\n",
    "        # dB\n",
    "        db = reshape_input.T.dot(dout)\n",
    "\n",
    "        db = np.sum(dout, axis=0)\n",
    "\n",
    "        return dx, [dw, db]\n",
    "\n",
    "    \n",
    "class flatten():\n",
    "    def __init__(self):\n",
    "        self.params = []\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out_shape = (x.shape[0], -1)\n",
    "        out = x.ravel().reshape(out_shape)\n",
    "        out_shape = out_shape[1]\n",
    "        self.x = x\n",
    "        return out\n",
    "\n",
    "    def backward(self, dout):\n",
    "        x = self.x\n",
    "        out = dout.reshape(x.shape)\n",
    "        return out, ()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exp_x = np.exp(x - np.max(x, axis=1, keepdims=True))\n",
    "    return exp_x / np.sum(exp_x, axis=1, keepdims=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crossEntropyLoss(X, y):\n",
    "    #X is the output from fully connected layer (num_examples x num_classes)\n",
    "    #y is labels (num_examples x 1)\n",
    "    m = y.shape[0]\n",
    "    p = softmax(X)\n",
    "    log_likelihood = -np.log(p[range(m), y])\n",
    "    loss = np.sum(log_likelihood) / m\n",
    "\n",
    "    #and its derivative\n",
    "    dx = p.copy()\n",
    "    dx[range(m), y] = dx[range(m), y] - 1\n",
    "    dx = dx / m\n",
    "    \n",
    "    return loss, dx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN:\n",
    "    def __init__(self, layers, loss_func=crossEntropyLoss):\n",
    "        self.layers = layers\n",
    "        self.params = []\n",
    "        for layer in self.layers:\n",
    "            self.params.append(layer.params)\n",
    "        self.loss_func = loss_func\n",
    "\n",
    "    def forward(self, X):\n",
    "        for layer in self.layers:\n",
    "            X = layer.forward(X)\n",
    "        return X\n",
    "\n",
    "    def backward(self, dout):\n",
    "        grads = []\n",
    "        for layer in reversed(self.layers):\n",
    "            dout, grad = layer.backward(dout)\n",
    "            grads.append(grad)\n",
    "        return grads\n",
    "\n",
    "    def train_step(self, X, y):\n",
    "        out = self.forward(X)\n",
    "        loss, dout = self.loss_func(out, y)\n",
    "        #loss += l2_regularization(self.layers)\n",
    "        grads = self.backward(dout)\n",
    "        #grads = delta_l2_regularization(self.layers, grads)\n",
    "        return loss, grads\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = self.forward(X)\n",
    "        return np.argmax(softmax(X), axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OPTIMIZER\n",
    "def accuracy(y_true, y_pred):\n",
    "    return np.mean(y_pred == y_true)  # both are not one hot encoded\n",
    "\n",
    "def minibatch(x, y, batch_size):\n",
    "    m = x.shape[0]\n",
    "    mini_batches = []\n",
    "    for i in range(0, m, batch_size):\n",
    "        x_batch = x[i:i+batch_size, :, :, :]\n",
    "        #print(x_batch.shape)\n",
    "        y_batch = y[i:i+batch_size, :]\n",
    "        mini_batches.append((x_batch, y_batch))\n",
    "    return mini_batches\n",
    "\n",
    "def vanilla_update(params, grads, learning_rate=0.001):\n",
    "    for param, grad in zip(params, reversed(grads)):\n",
    "        for i in range(len(grad)):\n",
    "            #print(\"i = \", i)\n",
    "            #print(\"param[i] = \", param[i])\n",
    "            #print(\"param[i].shape = \", param[i].shape)\n",
    "            #print(\"grad[i] = \", grad[i])\n",
    "            #print(\"grad[i].shape = \", grad[i].shape)\n",
    "            param[i] += -learning_rate*grad[i]\n",
    "            \n",
    "def sgd(nnet, xTrain, yTrain, batch_size, epoch, learning_rate, verbose=True, xTest=None, yTest=None):\n",
    "    minibatches = minibatch(xTrain, yTrain, batch_size)\n",
    "    for i in range(epoch):\n",
    "        loss = 0\n",
    "        if verbose:\n",
    "            print('Epoch {0}'.format(i+1))\n",
    "            for x_mini, y_mini in minibatches:\n",
    "                loss, grads = nnet.train_step(x_mini, y_mini)\n",
    "                vanilla_update(nnet.params, grads, learning_rate=learning_rate)\n",
    "            if verbose:\n",
    "                train_acc = accuracy(yTrain, nnet.predict(xTrain))\n",
    "                test_acc = accuracy(yTest, nnet.predict(xTest))\n",
    "                print('Loss = {0} | Training Accuracy = {1} | Test Accuracy {2}'.format(loss, train_acc, test_acc))\n",
    "    return nnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48000, 28, 28, 1)\n",
      "(48000, 1)\n",
      "(12000, 28, 28, 1)\n",
      "(12000, 1)\n"
     ]
    }
   ],
   "source": [
    "# Set random seed to Student ID\n",
    "seed = 105792018\n",
    "np.random.seed(seed)\n",
    "\n",
    "# Split data into train and validation sets\n",
    "# images_train 48000, 784\n",
    "# yval_train 48000, 1\n",
    "\n",
    "images_train, images_test, labels_train, labels_test = train_test_split(images,labels,test_size=0.2,shuffle=False)\n",
    "\n",
    "img_rows, img_cols = 28, 28\n",
    "\n",
    "images_train = np.reshape(images_train, (-1, img_rows, img_cols, 1))\n",
    "images_train = images_train.astype(np.float32)/255\n",
    "#print(images_train.shape)\n",
    "images_test = np.reshape(images_test, (-1, img_rows, img_cols, 1))\n",
    "images_test = images_test.astype(np.float32)/255\n",
    "X_dims = images_train.shape\n",
    "print(images_train.shape)\n",
    "print(labels_train.shape)\n",
    "print(images_test.shape)\n",
    "print(labels_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv Object is getting made\n",
      "\n",
      "self.H =  28\n",
      "self.W =  28\n",
      "self.C =  1\n"
     ]
    }
   ],
   "source": [
    "X_dims = (200, 28, 28, 1)\n",
    "conv1 = Conv(X_dims, kernel_w=3, kernel_h=3, kernel_c=1, F_out= 2, stride=1, padding=0)\n",
    "#self, X_dim, kernel_n, kernel_w, kernel_h, kernel_c, stride, padding\n",
    "#conv1.toString()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#print(\"XTrain shape = \", XTrain.shape)\n",
    "#X_Train = XTrain.reshape(XTrain.shape[0], XTrain.shape[2], XTrain.shape[3], XTrain.shape[1])\n",
    "#print(\"X_Train shape = \", X_Train.shape)\n",
    "#out = conv1.forward(X_Train)\n",
    "#out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#X_dim, size, stride\n",
    "#print(\"out.shape = \", out.shape)\n",
    "#pool1 = Maxpool(out.shape, 2,2)\n",
    "pool1 = Maxpool((200,26,26,2), 2, 2)\n",
    "#out2 = pool1.forward(out)\n",
    "#out2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conv Object is getting made\n",
      "\n",
      "self.H =  13\n",
      "self.W =  13\n",
      "self.C =  2\n"
     ]
    }
   ],
   "source": [
    "#self, X_dim, kernel_w, kernel_h, kernel_c, F, stride, padding\n",
    "\n",
    "conv2 = Conv((200,13,13,2), 3, 3, 2, 2, 1, 0)\n",
    "#conv2 = Conv(out2.shape, 3,3,2,2,1,0)\n",
    "#out3 = conv2.forward(out2)\n",
    "#out3.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "pool2 = Maxpool((200,11,11,2), 2, 2)\n",
    "#pool2 = Maxpool(out3.shape, 2, 2)\n",
    "#out4 = pool2.forward(out3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [],
   "source": [
    "#out4.shape\n",
    "flat = flatten()\n",
    "#flat1 = flat.forward()\n",
    "#flat1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc = fully_connected(50, 10)\n",
    "#fc1 = fc.forward(flat1)\n",
    "#fc1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = sigmoid()\n",
    "hyptanh = tanh()\n",
    "hyptanh2 = tanh()\n",
    "re = relu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = CNN([conv1, hyptanh, pool1, conv2, re, pool2, flat, fc, hyptanh2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "Loss = 460.5170185988091 | Training Accuracy = 0.09925 | Test Accuracy 0.103\n",
      "Epoch 2\n",
      "Loss = 460.5170185988091 | Training Accuracy = 0.09925 | Test Accuracy 0.103\n",
      "Epoch 3\n",
      "Loss = 460.5170185988091 | Training Accuracy = 0.09925 | Test Accuracy 0.103\n",
      "Epoch 4\n",
      "Loss = 460.5170185988091 | Training Accuracy = 0.09925 | Test Accuracy 0.103\n",
      "Epoch 5\n",
      "Loss = 460.5170185988091 | Training Accuracy = 0.09925 | Test Accuracy 0.103\n",
      "Epoch 6\n",
      "Loss = 460.5170185988091 | Training Accuracy = 0.09925 | Test Accuracy 0.103\n",
      "Epoch 7\n",
      "Loss = 460.5170185988091 | Training Accuracy = 0.09925 | Test Accuracy 0.103\n",
      "Epoch 8\n",
      "Loss = 460.5170185988091 | Training Accuracy = 0.09925 | Test Accuracy 0.103\n",
      "Epoch 9\n",
      "Loss = 460.5170185988091 | Training Accuracy = 0.09925 | Test Accuracy 0.103\n",
      "Epoch 10\n",
      "Loss = 460.5170185988091 | Training Accuracy = 0.09925 | Test Accuracy 0.103\n",
      "Epoch 11\n",
      "Loss = 460.5170185988091 | Training Accuracy = 0.09925 | Test Accuracy 0.103\n",
      "Epoch 12\n",
      "Loss = 460.5170185988091 | Training Accuracy = 0.09925 | Test Accuracy 0.103\n",
      "Epoch 13\n",
      "Loss = 460.5170185988091 | Training Accuracy = 0.09925 | Test Accuracy 0.103\n",
      "Epoch 14\n",
      "Loss = 460.5170185988091 | Training Accuracy = 0.09925 | Test Accuracy 0.103\n",
      "Epoch 15\n",
      "Loss = 460.5170185988091 | Training Accuracy = 0.09925 | Test Accuracy 0.103\n",
      "Epoch 16\n",
      "Loss = 460.5170185988091 | Training Accuracy = 0.09925 | Test Accuracy 0.103\n",
      "Epoch 17\n",
      "Loss = 460.5170185988091 | Training Accuracy = 0.09925 | Test Accuracy 0.103\n",
      "Epoch 18\n",
      "Loss = 460.5170185988091 | Training Accuracy = 0.09925 | Test Accuracy 0.103\n",
      "Epoch 19\n",
      "Loss = 460.5170185988091 | Training Accuracy = 0.09925 | Test Accuracy 0.103\n",
      "Epoch 20\n",
      "Loss = 460.5170185988091 | Training Accuracy = 0.09925 | Test Accuracy 0.103\n"
     ]
    }
   ],
   "source": [
    "#def sgd(nnet, xTrain, yTrain, batch_size, epoch, learning_rate, verbose=True, xTest=None, yTest=None):\n",
    "cnn_train = sgd(cnn, images_train, labels_train, 200, 20, 0.01, True, xTest=images_test, \n",
    "                yTest=labels_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 6, 6, ..., 6, 6, 6], dtype=int64)"
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cnn.predict(test_ims)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_ims = XTrain.reshape(48000,28,28,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
